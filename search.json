[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Methods in Behavioural Research",
    "section": "",
    "text": "Welcome to Advanced Methods in Behavioural Research!\nEach week’s lecture content is available via the VLE module page. These course pages are where you will find the materials for the practical sessions. Each practical session will have its own page of instructions released throughout the semester. Instructions can also be downloaded in Word/PDF format should you prefer.\nThis page outlines how each practical session is set up. Before you begin in Week 1, you should also check out the guidelines for setting up your workspace for this module."
  },
  {
    "objectID": "index.html#preparation",
    "href": "index.html#preparation",
    "title": "Advanced Methods in Behavioural Research",
    "section": "2.1 Preparation",
    "text": "2.1 Preparation\nWe expect that you will come to this session having attended the lecture on Monday, or caught up via the recording if you were unable to make it in person. We also expect that you will have done (at least) the core reading for the week to consolidate your understanding. Having done so will help you to engage fully with the practical session, and also enable you to ask questions about aspects that you did not understand."
  },
  {
    "objectID": "index.html#part-1-core-example",
    "href": "index.html#part-1-core-example",
    "title": "Advanced Methods in Behavioural Research",
    "section": "2.2 Part 1: Core example",
    "text": "2.2 Part 1: Core example\nThis first part of the practical will provide detailed instructions on how to conduct the relevant analysis steps.\n\nWork through the instructions to run the analysis yourself, checking your understanding and taking notes as you go.\nWhere available/appropriate, this section may make use of built-in datasets from R or the relevant packages. This isn’t us being lazy, it’s for very good reasons!\n\nFirst, these datasets are frequently used in tutorials and help files, meaning that building your familiarity with them will make it easier to understand online resources as you develop your understanding further.\nSecond, being able to use these datasets is a core part of being able to ask for help. Posting in online forums requires that you create a reproducible example of your issue - i.e., code that allows someone else to recreate the issue that you are experiencing. This is much easier if you can call upon an existing dataset that others can access too.\n\nThis section is the minimum amount that you are expected to work through within the session."
  },
  {
    "objectID": "index.html#part-2-application",
    "href": "index.html#part-2-application",
    "title": "Advanced Methods in Behavioural Research",
    "section": "2.3 Part 2: Application",
    "text": "2.3 Part 2: Application\nThe second section will instruct you to apply the analysis steps to a new question and/or dataset. This is designed to stretch your ability, and you may encounter some challenges along the way! If you don’t finish this within the practical session itself, you may want to continue this at home. You can ask any outstanding questions on the VLE."
  },
  {
    "objectID": "index.html#challenge-yourself",
    "href": "index.html#challenge-yourself",
    "title": "Advanced Methods in Behavioural Research",
    "section": "2.4 Challenge yourself!",
    "text": "2.4 Challenge yourself!\nIf you have completed both of the above, this last section encourages you to have a go at figuring something out for yourself."
  },
  {
    "objectID": "index.html#resolving-issues",
    "href": "index.html#resolving-issues",
    "title": "Advanced Methods in Behavioural Research",
    "section": "2.5 Resolving issues",
    "text": "2.5 Resolving issues\nAs you will already have learned, you can count on getting stuck at various points during the practical sessions. Errors are a very normal part of coding, even if sometimes frustrating! If you get stuck, try the following:\n\nSpend time trying some different things! What’s happening that you don’t expect? Is there an error message, and does it tell you anything useful?\nGoogle it! Copying and pasting the error message into Google often takes you to help forums where others have asked about the same problem.\nChat to the person next to you—maybe they can spot something in your code that you can’t see?\nAsk the session lead, we’re here to help!\n\n\n\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "ambr1_meta-analysis.html",
    "href": "ambr1_meta-analysis.html",
    "title": "1: Meta-analysis",
    "section": "",
    "text": "Learning objectives\n\n\n\nAs a reminder, the learning objectives for this week are as follows:\n\nExplain the purpose and value of meta-analytic approaches\nDescribe key stages of running a meta-analysis\nCompare different ways to weight and pool effect sizes, and implement them using the metafor package\nAssess and mitigate the impact of publication bias\nProduce and interpret standard visualisations of meta-analytic results\n\n\n\nThis week’s lecture introduced you to meta-analysis, a computational technique for combining results across several studies. We walked through the key steps involved in gathering data systematically, and outlined different ways that these can be combined in the statistical model.\nIn this practical session, we will focus primarily on what we do after the data has been gathered. How can we pool effect sizes according to our analytical choices, consider evidence for publication bias, and communicate our findings to a wider audience? We will start by using a ready loadable psychology dataset from the metadat package, then progress to a dataset curated here in York.",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#packages",
    "href": "ambr1_meta-analysis.html#packages",
    "title": "1: Meta-analysis",
    "section": "2.1 Packages",
    "text": "2.1 Packages\nToday’s practical will use the following packages. You will need to install each one, before loading them at the top of your script.\n\nlibrary(tidyverse)  # data wrangling tools \nlibrary(metadat)    # meta-analytic datasets\nlibrary(metafor)    # core meta-analysis tools",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#introduction-to-the-dataset",
    "href": "ambr1_meta-analysis.html#introduction-to-the-dataset",
    "title": "1: Meta-analysis",
    "section": "3.1 Introduction to the dataset",
    "text": "3.1 Introduction to the dataset\nWe will start by walking through the key analytic steps with a dataset that has already been curated. We will run an analysis to assess evidence in favour of the “Red-Romance Hypothesis” (Elliot & Niesta, 2008): a theory grounded in evolutionary and cultural psychology that the colour red enhances perceptions of attractiveness. The dataset was curated by Lehmann et al (2018), and released through the metadat package that we have loaded above.\nStart by loading it into your environment. Inspect the dataframe that you have loaded, and use the help files to understand the variables.\n\n# Load  metadat dataset into environment \nrr_dat &lt;- dat.lehmann2018\n\n# Inspect dataset information\nhelp(dat.lehmann2018)\n\nYou should be able to see that the dataset consists of a single effect size (yi) per row, with various other columns to code information about the study it was collected from.\nLet’s explore some data characteristics:\n\nUse nrow() to count the number of effect sizes (rows) in the dataset.\nYou can count the number of different studies using length(unique(rr_dat$Full_Citation).\nUse the range() or min()/max() functions to assess the range of sample sizes (Total.SampleSize) in the studies.\nCount the number of studies with male versus female participants using table(rr_dat$Gender).\nHow many studies were preregistered? How many used within versus between subject designs?\n\n\n\nShow the code\n# Number of effect sizes\nnrow(rr_dat)\n\n# Number of different studies\nlength(unique(rr_dat$Full_Citation))\n\n# Range of sample sizes\nrange(rr_dat$Total.SampleSize)\n\n# Number of male vs female\ntable(rr_dat$Gender)\n\n# Number preregistered vs not\ntable(rr_dat$Preregistered)\n\n# Number of within versus between subject designs\ntable(rr_dat$Design)\n\n\nLehmann et al’s meta-analysis considered the evidence separately for ‘males rating females’ versus ‘females rating males’. Let’s start by focusing on ‘males rating females’. Filter the dataset accordingly, and then use the forest() function to plot the effect sizes.\n\n# Select only effect sizes corresponding to male participants\nrr_m_ppts &lt;- filter(rr_dat, Gender == \"Males\")\n\n# Plot \nforest(rr_m_ppts$yi, rr_m_ppts$vi, slab = rr_m_ppts$Short_Title)\n\nWe have provided the forest() function with the list of standardised effect sizes, sampling variances, and list of study labels. How many studies look as if they show significant effects (confidence intervals that do not cross zero)? Are they all in the same direction?",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#computing-pooled-effect-sizes",
    "href": "ambr1_meta-analysis.html#computing-pooled-effect-sizes",
    "title": "1: Meta-analysis",
    "section": "3.2 Computing pooled effect sizes",
    "text": "3.2 Computing pooled effect sizes\nIn the lecture, you learned about fixed and random effects meta-analyses. Let’s have a go at running both.\n\n3.2.1 Fixed effects meta-analysis\nIn a fixed effects analysis, we assume that all studies are measuring the same underlying true effect size, and calculate the average effect size across the included studies. The metafor package terms this an “equal effects” model, as a better descriptive name. Use the rma() function to run this as follows:\n\n# Fixed effects model \nrr_fixed_m &lt;- rma(yi = yi, vi = vi,                  \n                 method = \"FE\", data = rr_m_ppts) \n\nWe’ve specified the variables with our effect size estimates and sampling variance, which have conveniently been named the same already by the authors. We’ve then specified the “FE” method for fixed effects, and the dataset to use.\nWe can use the summary() function on the model object to inspect the results.\n\nsummary(rr_fixed_m)\n\nThe output tells us that we ran a Fixed-Effects model with k = 45 effect sizes. The pooled estimate for the standardised mean difference is ~0.15, showing a small but significant effect of the colour red on men’s attractiveness ratings of women. However, it also tells us that there is substantial heterogeneity in study outcomes, indicating that the studies might be capturing varied effect sizes due to potential moderators or different study designs.\n\n\n3.2.2 Random effects meta-analysis\nThe heterogeneity indicates that a random effects model might be more appropriate— i.e., one that assumes that the samples in the meta-analysis are a random sample from a larger population of studies that could plausibly have been/will be conducted. This makes sense when we consider the study descriptions: they use a range of different outcome measures, and often include methodological variation within the same publications (as described in the study titles).\nWe can run this by simply changing the method in the meta-analysis function. The package provides us with several different options that have different ways of estimating the heterogeneity between studies (feel free to read more about these!), but for now we will stick with the default option.\n\n# Random effects model \nrr_random_m &lt;- rma(yi = yi, vi = vi,                 \n                  method = \"REML\", data = rr_m_ppts) \n\nUse the summary() function as above to inspect the model output. How has the pooled estimate for the standardised mean difference changed?\nWe can also inspect the results by including the pooled estimates on our forest plot. The forest() function can also take the meta-analysis model objects directly (rather than the separate lists of effect sizes and variances). Try this now with each of your models.\n\n\nShow the code\nforest(rr_fixed_m, slab = rr_m_ppts$Short_Title)\n\n\n\n\n\n\n\n\n\nShow the code\nforest(rr_random_m, slab = rr_m_ppts$Short_Title)",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#weighting-studies",
    "href": "ambr1_meta-analysis.html#weighting-studies",
    "title": "1: Meta-analysis",
    "section": "3.3 Weighting studies",
    "text": "3.3 Weighting studies\nWe saw when we loaded the dataset that the variation in sample size was very large. We should consider this in our analysis: a very large study likely provides a better estimate of the true effect size than a very small study.\n\n3.3.1 Under the hood: inverse variance\nOne of the most common ways to do this is via inverse-variance, which the rma() function does by default. If you inspect your rr_m_pptsdata frame, you will see that the smaller samples generally have the highest variance (vi). You might be able to see this more clearly via a quick scatterplot:\n\n# Inspect relationship between \nplot(rr_m_ppts$Total.SampleSize, rr_m_ppts$vi)\n\n# Now plot inverse variance instead\nplot(rr_m_ppts$Total.SampleSize, 1/rr_m_ppts$vi)\n\nTo use this variance in the analysis, the model uses the inverse (1/variance), so that smaller samples have the smallest weighting. The function does this under the hood, but you can see what this looks like by computing it for yourself. Recreate your plot using 1/rr_m_ppts$vi as your y variable.\nYou should now see a steady positive trend for the majority of studies, with higher inverse variance for large studies. A few studies buck this trend, with little variance in their estimates despite small-medium sample sizes. Can you identify them in the dataset?\n\n\n3.3.2 Changing weights in the meta-analysis\nBecause weighting by inverse variance is a very common and sensible approach, the rma() meta-analysis function does this by default. This means you’ve already weighted your samples in your above models!\nTo compare, try fitting two new models (fixed, random) and include the argument weighted = FALSE. Use a different object name to store the model output, so that you can inspect the differences in output when you call summary().\nAlternatively, you can try weighting by sample size alone, by setting weights = N.",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#inspecting-for-publication-bias",
    "href": "ambr1_meta-analysis.html#inspecting-for-publication-bias",
    "title": "1: Meta-analysis",
    "section": "3.4 Inspecting for publication bias",
    "text": "3.4 Inspecting for publication bias\nFinally, let’s consider whether this analysis might be affected by publication bias. Let’s use the random effects model given it would be the more appropriate choice here, and create a funnel plot:\n\nfunnel(rr_random_m)\n\nTake a look at this linked guide for interpreting the funnel plots. What do you conclude?\nYou can also test for asymmetry using Egger’s regression test. Is there statistical evidence for assymmetry?\n\nregtest(rr_random_m)\n\nFor comparison, inspect the evidence for publication bias using female participants instead. You’ll need to subset the data, run a new random effects model, and then inspect for asymmetry. What do you conclude about publication bias in this area of research?\n\n\nShow the code\n# Select only effect sizes corresponding to female participants\nrr_f_ppts &lt;- filter(rr_dat, Gender == \"Females\")\nrr_random_f &lt;- rma(yi = yi, vi = vi, data = rr_f_ppts)\nfunnel(rr_random_f)\nregtest(rr_random_f)\n\n\n\n\n\n\n\n\n\nTake a break!\n\n\n\nIf you haven’t already taken a break today, we suggest you so before you start the next section.",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#introduction-to-the-dataset-1",
    "href": "ambr1_meta-analysis.html#introduction-to-the-dataset-1",
    "title": "1: Meta-analysis",
    "section": "4.1 Introduction to the dataset",
    "text": "4.1 Introduction to the dataset\nLet’s jump in with a new dataset! This time we will use a recently published meta-analysis that examined the relationship between procedural learning and reading ability (Oliveira et al., 2023).\nThe Procedural Deficit Hypothesis proposes that reading and language disorders stem from a difficulty in the procedural learning system - i.e., the system that underpins the learning of probabilistic knowledge (Ullman et al., 2020). Procedural learning has often been measured using Serial Reaction Time (SRT) tasks: participants respond to a repeating sequence of stimuli, and their improvements in speed over several blocks are measured as an index of learning. However, evidence that SRT performance is associated with reading ability is mixed, with some studies showing a strong association and others finding nothing at all. Many studies in this area suffer from small sample sizes, making it hard to draw sound conclusions.\nThis meta-analysis was conducted by a former PhD student at York, Dr Catia Oliveira, together with Professor Lisa Henderson and Dr Emma Hayiou-Thomas. We will walk through one new step of the process together in extracting effect sizes and calculating pooled variances. Then you will apply your new meta-analysis skills.",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#updating-the-dataset",
    "href": "ambr1_meta-analysis.html#updating-the-dataset",
    "title": "1: Meta-analysis",
    "section": "4.2 Updating the dataset",
    "text": "4.2 Updating the dataset\nOliveira et al. made their protocol and analyses openly available on the Open Science Framework. This means that we can directly download the data into R as follows.\n\n# Download original dataset\nsrt_dat &lt;- read.csv(\"https://osf.io/yj9bs/?action=download\")\n\nLet’s focus just on the relationship between SRT and reading, for simplicity, and stick to word/nonword reading tasks. We’re going to add a new study to the dataset, so for the sake of this practical we will also strip back the dataset to a few key variables (otherwise adding the new dataset will take you forever!).\n\nsrt_dat2 &lt;- srt_dat %&gt;% \n  filter(Component == \"Reading\") %&gt;% \n  filter(Task == \"Word Reading\" | Task == \"Nonword Reading\") %&gt;% \n  select(Study, N, Age, Task, Group, Cor)\n\nUse your R skills to answer the following questions about the dataset:\n\nHow many effect sizes are there?\nHow many unique studies do they come from?\nWhat is the range of average ages?\nHow many samples are from the different participant groups?\n\n\n\nShow the code\n# Number of effect sizes \nnrow(srt_dat2)\n\n# Number of unique studies\nlength(unique(srt_dat2$Study))\n\n# Range of ages \nrange(srt_dat2$Age)\n\n# Effect sizes from different participant groups \ntable(srt_dat2$Group)\n\n\nThis meta-analysis included data that were published up until November 2020, but many studies on this topic have been published since then. Let’s add this study published by van Witteloostuijn et al. (2021), which adds 2 new correlation effect sizes between SRT and word/nonword reading, for each of two groups (typically developing children, children with developmental dyslexia). Use Appendix 1 of the paper to replace the NAs below with the relevant numbers, and run the code to add 4 new rows to the dataset.\n\nsrt_dat3 &lt;- srt_dat2 %&gt;% \n  \n  add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = NA,\n          Age = 9.66,\n          Task = \"Word Reading\",\n          Group = \"TD\", \n          Cor = NA) %&gt;% \n  \n    add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = NA,\n          Age = 9.66,\n          Task = \"Nonword Reading\",\n          Group = \"TD\", \n          Cor = NA) %&gt;% \n  \n    add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = NA,\n          Age = 9.83,\n          Task = \"Word Reading\",\n          Group = \"DD\", \n          Cor = NA) %&gt;% \n  \n    add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = NA,\n          Age = 9.83,\n          Task = \"Nonword Reading\",\n          Group = \"DD\", \n          Cor = NA)",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#compute-sampling-variance",
    "href": "ambr1_meta-analysis.html#compute-sampling-variance",
    "title": "1: Meta-analysis",
    "section": "4.3 Compute sampling variance",
    "text": "4.3 Compute sampling variance\nIn the dataset above, the effect sizes and sampling variances had already been calculated for us. In the current dataset, we have extracted a consistent measure of effect size (the correlation coefficient representing the relationship between SRT task performance and reading ability), but still need to compute the sampling variance around the estimates to capture uncertainty. The metafor package provides a handy function for doing this: simply replace the NAs in the below code.\n\nsrt_dat3 &lt;- escalc(measure = \"COR\",   # specify effect size measure         \n                   ri = NA,           # specify correlation variable \n                   ni = NA,           # specify sample size variable \n                   data = srt_dat3)   # dataset\n\nYou should now see two new columns nicely formatted for the meta analysis:\n\nyi: your effect size, which in this case is identical to the one in the Cor column as we haven’t asked it to do any kind of transformation\nvi: your sampling variances, which have now been computed based on the correlation coefficient and sample size\n\nNow you’re ready to go!",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#conduct-your-meta-analysis",
    "href": "ambr1_meta-analysis.html#conduct-your-meta-analysis",
    "title": "1: Meta-analysis",
    "section": "4.4 Conduct your meta-analysis",
    "text": "4.4 Conduct your meta-analysis\nYou can now finish conducting your meta-analysis. Conduct the following steps, and write notes to justify your decisions along the way:\n\nDecide on the appropriate model and weighting strategy\nRun the model and interpret the output\nCreate a forest plot\nEvaluate evidence for publication bias\n\n\n\nShow the code\n# Random effects model\nsrt_mod &lt;- rma(yi = yi, vi = vi,\n                method = \"REML\", data = srt_dat3)\nsummary(srt_mod)\n\n# Create forest plot \nforest(srt_mod, slab = srt_dat3$Study)\n\n# Examine evidence for publication bias\nfunnel(srt_mod)\nregtest(srt_mod)",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#recap",
    "href": "ambr1_meta-analysis.html#recap",
    "title": "1: Meta-analysis",
    "section": "6.1 Recap",
    "text": "6.1 Recap\nThis week, we’ve seen how meta-analyses can be used to synthesise evidence across a range of different studies to answer a research question. The starting point of any meta-analysis is a systematic literature search—although this would have been far too large a task for today’s practical! From there, we’ve learned to extract effect sizes, calculate sampling variances, and pool them according to principled decisions about how they should be weighted and combined in the model. We saw how to inspect for possible publication bias, and create gold-standard visualisations of the results. Well done!\nWe ran a basic version of these models here, but there is much more you can do to correct for small sample sizes and adjust for the fact that some estimates come from the same study (i.e., there were different correlation coefficients for different measures of reading in the same sample). If you are interested in these issues, you can access the full paper here or download the full dataset and analysis scripts.",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr1_meta-analysis.html#further-learning",
    "href": "ambr1_meta-analysis.html#further-learning",
    "title": "1: Meta-analysis",
    "section": "6.2 Further learning",
    "text": "6.2 Further learning\nMake sure that you have completed the core reading for this week, and consider reading further resources to consolidate your understanding. As you’ll have seen today, the code for actually running analyses is quite sparse(!), but it’s important that you read around the decisions that need to be made along the way.\nIf you come to do meta-analysis in the future or want further details on the different steps that we have covered, then there is an excellent textbook available for free online: Harrer et al. (2021). Doing Meta-Analysis with R: A Hands-On Guide.\n\n\n\n\n\n\nFeedback!\n\n\n\nThese practical sessions are new this year. Please take a moment to rate the difficulty and length of these practical activities via this survey, and let us know of any issues you encountered or aspects you didn’t understand (positive feedback is also welcome!). You will need to be logged in with your university account to respond, but your submission will be anonymous.\nFor more general questions about the practical, please post them on the VLE Discussion Board.",
    "crumbs": [
      "Week 1: Meta-analysis"
    ]
  },
  {
    "objectID": "ambr0_guide.html",
    "href": "ambr0_guide.html",
    "title": "Working in this module",
    "section": "",
    "text": "This section provides some guidelines and recommendations for organising your work as we progress throughout the module. However, your development as a data scientist is now well underway, and you may have already established preferred ways of working. This is fine. Consider the guidance below and decide on your workflow for this module, and default to the recommendations if you are not yet confident in your set-up.",
    "crumbs": [
      "Guide to working in this module"
    ]
  },
  {
    "objectID": "ambr0_guide.html#set-up-your-folders",
    "href": "ambr0_guide.html#set-up-your-folders",
    "title": "Working in this module",
    "section": "1.1 Set up your folders",
    "text": "1.1 Set up your folders\nYou can do this from within R if you prefer, but here are the simplest instructions for setting up on your device.\n1) Open the file browser on your computer. On Windows, you can do this by clicking Start &gt;&gt; File Explorer.\n2) Navigate to your Documents folder.\n3) Create a new folder, and call it something like “AMBR_Practicals”.\n4) Within this new folder, create one folder called “scripts”, and another folder called “data”.\n\n\n\n\n\n\nTip\n\n\n\nNote that R is case sensitive, meaning that if you capitalise letters here then you will also need to be specific in your capitalisation in the scripts. It is a good idea to adopt a consistent approach to avoid later issues. Here we have used lower case, and strongly advise you to do the same so that you don’t get confused later on.\n\n\nWhen you download datasets for the practicals in the future, you should save them in your data folder. Similarly, when you create scripts in each practical, save them in the scripts folder.",
    "crumbs": [
      "Guide to working in this module"
    ]
  },
  {
    "objectID": "ambr0_guide.html#initiate-an-r-project",
    "href": "ambr0_guide.html#initiate-an-r-project",
    "title": "Working in this module",
    "section": "1.2 Initiate an R Project",
    "text": "1.2 Initiate an R Project\nNow let’s tell R where we want to work during these practicals.\n1) Open RStudio.\n2) At the top of the window, click File &gt;&gt; New Project…\n3) After a bit of thinking (be patient!), a “New Project Wizard” window should appear. Click the Existing Directory option in the middle, to associate this new project with an existing working directory. We choose this option because we have already set up our folders manually in the previous step.\n4) On the next window, click “Browse…” and then navigate to your AMBR_Practicals folder. Once you are in it (and can see your data/scripts folders), click “Open”.\n5) Click “Create Project”.\nThe name of the R Project you are working in should now show in the top right hand corner of your window, like this:\n\n\n\nThe current R Project\n\n\nIf it doesn’t, retrace your steps and try again. If it still doesn’t work, ask us for help.",
    "crumbs": [
      "Guide to working in this module"
    ]
  },
  {
    "objectID": "ambr0_guide.html#moving-forwards",
    "href": "ambr0_guide.html#moving-forwards",
    "title": "Working in this module",
    "section": "1.3 Moving forwards",
    "text": "1.3 Moving forwards\nThe instructions for the practicals will assume that you have set up R in this way. We will follow three key principles:\n1) Open the R Project at the start of every session. You can do this by double clicking it from your file browser, or clicking File &gt;&gt; Open Project… from within RStudio.\n2) Save the data files in the data folder, once you have downloaded them.\n3) Save your scripts in the scripts folder.",
    "crumbs": [
      "Guide to working in this module"
    ]
  },
  {
    "objectID": "ambr0_guide.html#saving-the-workspace",
    "href": "ambr0_guide.html#saving-the-workspace",
    "title": "Working in this module",
    "section": "3.1 Saving the workspace",
    "text": "3.1 Saving the workspace\nOne irritating default of RStudio is that it saves everything automatically on exiting. This might sound like a great idea, but in reality it can mean that you have a very messy workspace and that mistakes can emerge from using old material. To turn this off (highly recommended):\n1) At the top of RStudio, click Tools &gt;&gt; Global Options…\n2) On the General panel that should open in the window, there is a section titled Workspace.\n3) Make sure that the Restore .RData[…] box is unticked, and that it is set to never save the workspace on exit.\n4) Click “Apply” to save the settings.\n\n\n\nGlobal options window with recommended General settings",
    "crumbs": [
      "Guide to working in this module"
    ]
  },
  {
    "objectID": "ambr0_guide.html#rainbow-parentheses",
    "href": "ambr0_guide.html#rainbow-parentheses",
    "title": "Working in this module",
    "section": "3.2 Rainbow parentheses",
    "text": "3.2 Rainbow parentheses\nSometimes we will need to wrap things inside multiple nested brackets. You don’t need to worry about what this means for now, but whilst we’re on the settings, we might as well set up one of my favourite most useful features: rainbow parentheses! This setting colour codes your brackets so that you can tell which ones are paired together. And who doesn’t want a bit of colour on their scripts?!\n1) On the Global Options… window you have open, click on “Code” down the left hand side.\n2) Click the “Display” tab at the op.\n3) Under syntax, check the box that says “Use rainbow parentheses”.\n4) Then click “Apply”, and/or “OK” to close the window.\n\n\n\nGlobal options window with recommended Code Display settings\n\n\nAs for what this has done? Well, you’ll just have to wait and see as we get writing scripts…",
    "crumbs": [
      "Guide to working in this module"
    ]
  },
  {
    "objectID": "ambrX_template.html",
    "href": "ambrX_template.html",
    "title": "[Week X: template]",
    "section": "",
    "text": "DELETE THIS: Template instructions\n\n\n\n\nEdit the title and author above, following the format of the other sessions\nDo any necessary data preprocessing in the housekeeping chunk below (and leave this in the script)\nNote that the headings will be numbered once rendered - using heading level 2 for each major step of the analysis in the two example sections, and lower heading levels as appropriate"
  },
  {
    "objectID": "ambrX_template.html#recap",
    "href": "ambrX_template.html#recap",
    "title": "[Week X: template]",
    "section": "4.1 Recap",
    "text": "4.1 Recap\n[summarise what they have achieved in relation to the learning objectives]"
  },
  {
    "objectID": "ambrX_template.html#further-learning",
    "href": "ambrX_template.html#further-learning",
    "title": "[Week X: template]",
    "section": "4.2 Further learning",
    "text": "4.2 Further learning\nMake sure that you have completed the core reading for this week, and consider reading further resources to consolidate your understanding.\nFor further practical exercises, you may be interested in the following:\n[list DataCamp courses and/or other online tutorials]\n\n\n\n\n\n\nFeedback!\n\n\n\nThese practical sessions are new this year. Please take a moment to rate the difficulty and length of these practical activities via this survey, and let us know of any issues you encountered or aspects you didn’t understand (positive feedback is also welcome!). You will need to be logged in with your university account to respond, but your submission will be anonymous.\nFor more general questions about the practical, please post them on the VLE Discussion Board."
  }
]