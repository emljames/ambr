{
  "hash": "1f140da0b4bbf394ed181e2c76a94466",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 1: Meta-analysis\"\nauthor: \"Emma James\"\neditor: visual\n---\n\n\n\n# Introduction\n\n::: {.callout-note title=\"Learning objectives\"}\nAs a reminder, the learning objectives for this week are as follows:\n\n1.  Explain the purpose and value of meta-analytic approaches\n2.  Describe key stages of running a meta-analysis\n3.  Compare different ways to weight and pool effect sizes, and implement them using the *metafor* package\n4.  Assess and mitigate the impact of publication bias\n5.  Produce and interpret standard visualisations of meta-analytic results\n:::\n\nThis week's lecture introduced you to *meta-analysis,* a computational technique for combining results across several studies. We walked through the key steps involved in gathering data systematically, and outlined different ways that these can be combined in the statistical model.\n\nIn this practical session, we will focus primarily on what we do *after* the data has been gathered. How can we pool effect sizes according to our analytical choices, consider evidence for publication bias, and communicate our findings to a wider audience? We will start by using a ready loadable psychology dataset from the *metadat* package, then progress to a dataset curated here in York.\n\n------------------------------------------------------------------------\n\n# Setting up\n\n## Packages\n\nToday's practical will use the following packages. You will need to install each one, before loading them at the top of your script.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)  # data wrangling tools \nlibrary(metadat)    # meta-analytic datasets\nlibrary(metafor)    # core meta-analysis tools\n```\n:::\n\n\n------------------------------------------------------------------------\n\n# Part A: Core example\n\n## Introduction to the dataset\n\nWe will start by walking through the key analytic steps with a dataset that has already been curated. We will run an analysis to assess evidence in favour of the \"Red-Romance Hypothesis\" (Elliot & Niesta, 2008): a theory grounded in evolutionary and cultural psychology that the colour red enhances perceptions of attractiveness. The dataset was curated by [Lehmann et al (2018)](https://journals.sagepub.com/doi/10.1177/1474704918802412), and released through the `metadat` package that we have loaded above.\n\nStart by loading it into your environment. Inspect the dataframe that you have loaded, and use the help files to understand the variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load  metadat dataset into environment \nrr_dat <- dat.lehmann2018\n\n# Inspect dataset information\nhelp(dat.lehmann2018)\n```\n:::\n\n\nYou should be able to see that the dataset consists of a single effect size (`yi`) per row, with various other columns to code information about the study it was collected from.\n\nLet's explore some data characteristics:\n\n-   Use `nrow()` to count the number of effect sizes (rows) in the dataset.\n-   You can count the number of different studies using `length(unique(rr_dat$Full_Citation)`.\n-   Use the `range()` or `min()`/`max()` functions to assess the range of sample sizes (`Total.SampleSize`) in the studies.\n-   Count the number of studies with male versus female participants using `table(rr_dat$Gender)`.\n-   How many studies were preregistered? How many used within versus between subject designs?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Number of effect sizes\nnrow(rr_dat)\n\n# Number of different studies\nlength(unique(rr_dat$Full_Citation))\n\n# Range of sample sizes\nrange(rr_dat$Total.SampleSize)\n\n# Number of male vs female\ntable(rr_dat$Gender)\n\n# Number preregistered vs not\ntable(rr_dat$Preregistered)\n\n# Number of within versus between subject designs\ntable(rr_dat$Design)\n```\n:::\n\n\nLehmann et al's meta-analysis considered the evidence separately for 'males rating females' versus 'females rating males'. Let's start by focusing on 'males rating females'. Filter the dataset accordingly, and then use the `forest()` function to plot the effect sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select only effect sizes corresponding to male participants\nrr_m_ppts <- filter(rr_dat, Gender == \"Males\")\n\n# Plot \nforest(rr_m_ppts$yi, rr_m_ppts$vi, slab = rr_m_ppts$Short_Title)\n```\n:::\n\n\nWe have provided the `forest()` function with the list of standardised effect sizes, sampling variances, and list of study labels. How many studies look as if they show significant effects (confidence intervals that do not cross zero)? Are they all in the same direction?\n\n## Computing pooled effect sizes\n\nIn the lecture, you learned about fixed and random effects meta-analyses. Let's have a go at running both.\n\n### Fixed effects meta-analysis\n\nIn a fixed effects analysis, we assume that all studies are measuring the same underlying true effect size, and calculate the average effect size across the included studies. The `metafor` package terms this an \"equal effects\" model, as a better descriptive name. Use the `rma()` function to run this as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fixed effects model \nrr_fixed_m <- rma(yi = yi, vi = vi,                  \n                 method = \"FE\", data = rr_m_ppts) \n```\n:::\n\n\nWe've specified the variables with our effect size estimates and sampling variance, which have conveniently been named the same already by the authors. We've then specified the \"FE\" method for fixed effects, and the dataset to use.\n\nWe can use the `summary()` function on the model object to inspect the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(rr_fixed_m)\n```\n:::\n\n\nThe output tells us that we ran a Fixed-Effects model with *k* = 45 effect sizes. The pooled estimate for the standardised mean difference is \\~0.15, showing a small but significant effect of the colour red on men's attractiveness ratings of women. However, it also tells us that there is substantial heterogeneity in study outcomes, indicating that the studies might be capturing varied effect sizes due to potential moderators or different study designs.\n\n### Random effects meta-analysis\n\nThe heterogeneity indicates that a random effects model might be more appropriate— i.e., one that assumes that the samples in the meta-analysis are a random sample from a larger population of studies that could plausibly have been/will be conducted. This makes sense when we consider the study descriptions: they use a range of different outcome measures, and often include methodological variation within the same publications (as described in the study titles).\n\nWe can run this by simply changing the method in the meta-analysis function. The package provides us with several different options that have different ways of estimating the heterogeneity between studies (feel free to read more about these!), but for now we will stick with the default option.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random effects model \nrr_random_m <- rma(yi = yi, vi = vi,                 \n                  method = \"REML\", data = rr_m_ppts) \n```\n:::\n\n\nUse the `summary()` function as above to inspect the model output. How has the pooled estimate for the standardised mean difference changed?\n\nWe can also inspect the results by including the pooled estimates on our forest plot. The `forest()` function can also take the meta-analysis model objects directly (rather than the separate lists of effect sizes and variances). Try this now with each of your models.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nforest(rr_fixed_m, slab = rr_m_ppts$Short_Title)\nforest(rr_random_m, slab = rr_m_ppts$Short_Title)\n```\n:::\n\n\n## Weighting studies\n\nWe saw when we loaded the dataset that the variation in sample size was very large. We should consider this in our analysis: a very large study likely provides a better estimate of the true effect size than a very small study.\n\n### Under the hood: inverse variance\n\nOne of the most common ways to do this is via inverse-variance, which the `rma()` function does by default. If you inspect your `rr_m_ppts`data frame, you will see that the smaller samples generally have the highest variance (`vi`). You might be able to see this more clearly via a quick scatterplot *(NB: I've done this quickly using base R functions for a quick check, but feel free to use ggplot if you prefer!)*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inspect relationship between \nplot(rr_m_ppts$Total.SampleSize, rr_m_ppts$vi)\n\n# Now plot inverse variance instead\nplot(rr_m_ppts$Total.SampleSize, 1/rr_m_ppts$vi)\n```\n:::\n\n\nTo use this variance in the analysis, the model uses the *inverse* (`1/variance`), so that smaller samples have the smallest weighting. The function does this under the hood, but you can see what this looks like by computing it for yourself. Recreate your plot using `1/rr_m_ppts$vi` as your y variable.\n\nYou should now see a steady positive trend for the majority of studies, with higher inverse variance for large studies. A few studies buck this trend, with little variance in their estimates despite small-medium sample sizes. Can you identify them in the dataset?\n\n### Changing weights in the meta-analysis\n\nBecause weighting by inverse variance is a very common and sensible approach, the `rma()` meta-analysis function does this by default. This means you've already weighted your samples in your above models!\n\nTo compare, try fitting two new models (fixed, random) and include the argument `weighted = FALSE`. Use a different object name to store the model output, so that you can inspect the differences in output when you call `summary()`.\n\nAlternatively, you can try weighting by sample size alone, by setting `weights = N`.\n\n## Inspecting for publication bias\n\nFinally, let's consider whether this analysis might be affected by publication bias. Let's use the random effects model given it would be the more appropriate choice here, and create a funnel plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunnel(rr_random_m)\n```\n:::\n\n\nTake a look at [this linked guide for interpreting the funnel plots](https://www.statisticshowto.com/funnel-plot/). What do you conclude?\n\nYou can also test for asymmetry using Egger's regression test. Is there statistical evidence for assymmetry?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregtest(rr_random_m)\n```\n:::\n\n\nFor comparison, inspect the evidence for publication bias using female participants instead. You'll need to subset the data, run a new random effects model, and then inspect for asymmetry. What do you conclude about publication bias in this area of research?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Select only effect sizes corresponding to female participants\nrr_f_ppts <- filter(rr_dat, Gender == \"Females\")\nrr_random_f <- rma(yi = yi, vi = vi, data = rr_f_ppts)\nfunnel(rr_random_f)\nregtest(rr_random_f)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n::: callout-tip\n# Take a break!\n\nIf you haven't already taken a break today, we suggest you so before you start the next section.\n\n![](images/break.png){fig-alt=\"Take a break\" fig-align=\"center\" width=\"227\"}\n:::\n\n------------------------------------------------------------------------\n\n# Part B: Apply your knowledge\n\n## Introduction to the dataset\n\nLet's jump in with a new dataset! This time we will use a recently published meta-analysis that examined the relationship between procedural learning and reading ability ([Oliveira et al., 2023](https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13310)).\n\nThe Procedural Deficit Hypothesis proposes that reading and language disorders stem from a difficulty in the procedural learning system - i.e., the system that underpins the learning of probabilistic knowledge (Ullman et al., 2020). Procedural learning has often been measured using Serial Reaction Time (SRT) tasks: participants respond to a repeating sequence of stimuli, and their improvements in speed over several blocks are measured as an index of learning. However, evidence that SRT performance is associated with reading ability is mixed, with some studies showing a strong association and others finding nothing at all. Many studies in this area suffer from small sample sizes, making it hard to draw sound conclusions.\n\nThis meta-analysis was conducted by a former PhD student at York, Dr Catia Oliveira, together with Professor Lisa Henderson and Dr Emma Hayiou-Thomas. We will walk through one new step of the process together in extracting effect sizes and calculating pooled variances. Then you will apply your new meta-analysis skills.\n\n## Updating the dataset\n\nOliveira et al. made their protocol and analyses openly available on the [Open Science Framework](https://osf.io/ev2xw/overview). This means that we can directly download the data into R as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download original dataset\nsrt_dat <- read_csv(\"https://osf.io/yj9bs/?action=download\")\n```\n:::\n\n\nLet's focus just on the relationship between SRT and reading, for simplicity, and stick to word/nonword reading tasks. We're going to add a new study to the dataset, so for the sake of this practical we will also strip back the dataset to a few key variables (otherwise adding the new dataset will take you forever!).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsrt_dat2 <- srt_dat %>% \n  filter(Component == \"Reading\") %>% \n  filter(Task == \"Word Reading\" | Task == \"Nonword Reading\") %>% \n  select(Study, N, Age, Task, Group, Cor)\n```\n:::\n\n\nUse your R skills to answer the following questions about the dataset:\n\n-   How many effect sizes are there?\n-   How many unique studies do they come from?\n-   What is the range of average ages?\n-   How many samples are from the different participant groups?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Number of effect sizes \nnrow(srt_dat2)\n\n# Number of unique studies\nlength(unique(srt_dat2$Study))\n\n# Range of ages \nrange(srt_dat2$Age)\n\n# Effect sizes from different participant groups \ntable(srt_dat2$Group)\n```\n:::\n\n\nThis meta-analysis included data that were published up until November 2020, but many studies on this topic have been published since then. Let's add this study published by [van Witteloostuijn et al. (2021)](https://onlinelibrary.wiley.com/doi/full/10.1002/dys.1678), which adds 2 new correlation effect sizes between SRT and word/nonword reading, for each of two groups (typically developing children, children with developmental dyslexia). Copy this section of code below into your script, and then use Appendix 1 of the paper to replace the NAs below with the relevant numbers. Run the code to add 4 new rows to the dataset.\n\n\n::: {.cell code-copy='true'}\n\n```{.r .cell-code}\nsrt_dat3 <- srt_dat2 %>% \n  \n  add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = NA,\n          Age = 9.66,\n          Task = \"Word Reading\",\n          Group = \"TD\", \n          Cor = NA) %>% \n  \n    add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = NA,\n          Age = 9.66,\n          Task = \"Nonword Reading\",\n          Group = \"TD\", \n          Cor = NA) %>% \n  \n    add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = NA,\n          Age = 9.83,\n          Task = \"Word Reading\",\n          Group = \"DD\", \n          Cor = NA) %>% \n  \n    add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = NA,\n          Age = 9.83,\n          Task = \"Nonword Reading\",\n          Group = \"DD\", \n          Cor = NA)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nsrt_dat3 <- srt_dat2 %>% \n  \n  add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = 50,\n          Age = 9.66,\n          Task = \"Word Reading\",\n          Group = \"TD\", \n          Cor = -0.17) %>% \n  \n    add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = 50,\n          Age = 9.66,\n          Task = \"Nonword Reading\",\n          Group = \"TD\", \n          Cor = -0.26) %>% \n  \n    add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = 50,\n          Age = 9.83,\n          Task = \"Word Reading\",\n          Group = \"DD\", \n          Cor = 0.37) %>% \n  \n    add_row(Study = \"van Witteloostuijn et al. (2021)\",\n          N = 50,\n          Age = 9.83,\n          Task = \"Nonword Reading\",\n          Group = \"DD\", \n          Cor = 0.26)\n```\n:::\n\n\n## Compute sampling variance\n\nIn the dataset above, the effect sizes and sampling variances had already been calculated for us. In the current dataset, we have extracted a consistent measure of effect size (the correlation coefficient representing the relationship between SRT task performance and reading ability), but still need to compute the sampling variance around the estimates to capture uncertainty. The *metafor* package provides a handy function for doing this: simply replace the NAs in the below code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsrt_dat3 <- escalc(measure = \"COR\",   # specify effect size measure         \n                   ri = NA,           # specify correlation variable \n                   ni = NA,           # specify sample size variable \n                   data = srt_dat3)   # dataset\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nsrt_dat3 <- escalc(measure = \"COR\",   # specify effect size measure         \n                   ri = Cor,          # specify correlation variable \n                   ni = N,            # specify sample size variable \n                   data = srt_dat3)   # dataset\n```\n:::\n\n\nYou should now see two new columns nicely formatted for the meta analysis:\n\n-   **yi*:*** your effect size, which in this case is identical to the one in the Cor column as we haven't asked it to do any kind of transformation\n-   **vi:** your sampling variances, which have now been computed based on the correlation coefficient and sample size\n\nNow you're ready to go!\n\n## Conduct your meta-analysis\n\nYou can now finish conducting your meta-analysis. Conduct the following steps, and write notes to justify your decisions along the way:\n\n-   Decide on the appropriate model and weighting strategy\n-   Run the model and interpret the output\n-   Create a forest plot, and see if you can customise it (e.g., change the colour of the polygone, print the study weights)\n-   Evaluate evidence for publication bias\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Random effects model, as different populations and tasks - likely a distribution\n# Default study weighting appropriate\nsrt_mod <- rma(yi = yi, vi = vi,\n                method = \"REML\", data = srt_dat3)\nsummary(srt_mod)\n\n# Create forest plot \nforest(srt_mod, slab = srt_dat3$Study)\n\n# Examine evidence for publication bias\nfunnel(srt_mod)\nregtest(srt_mod)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n# Part C: Challenge yourself!\n\nOften we are interested in how effect sizes vary across different conditions. In this case, the researchers were interested in whether the relationship between procedural learning (SRT) and reading ability might differ depending on the participant groups tested.\n\nIf you finish the practical in good time or want to push your skills further beyond the end of today's session, see if you can test whether participant `Group` is a moderator in your analysis. How much additional variance do the groups account for? In which group(s) are the relationships between SRT and reading ability observed?\n\nYou can look at the help files or use Google to find out how to include this extra predictor.\n\n::: {.callout-tip collapse=\"true\"}\n## Click for a hint!\n\nYou will need to start by recoding your Group variable as a factor, setting \"TD\" as the baseline for comparison.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\n# Reformat group as a factor, with TD as the reference level\nsrt_dat4 <- srt_dat3 %>% \n  mutate(Group_f = factor(Group, levels = c(\"TD\", \"DD\", \"DLD\")))\n\n# Run a new model with Group as a moderator\nmod_mod <- rma(yi = yi, vi = vi, mods = ~factor(Group_f),\n                method = \"REML\", data = srt_dat4)\n\n# Inspect the output\nsummary(mod_mod)\n```\n:::\n\n\n------------------------------------------------------------------------\n\n# Summary\n\n## Recap\n\nThis week, we've seen how meta-analyses can be used to synthesise evidence across a range of different studies to answer a research question. The starting point of any meta-analysis is a systematic literature search—although this would have been far too large a task for today's practical! From there, we've learned to extract effect sizes, calculate sampling variances, and pool them according to principled decisions about how they should be weighted and combined in the model. We saw how to inspect for possible publication bias, and create gold-standard visualisations of the results. Well done!\n\nWe ran a basic version of these models here, but there is much more you can do to correct for small sample sizes and adjust for the fact that some estimates come from the same study (i.e., there were different correlation coefficients for different measures of reading in the same sample). If you are interested in these issues, you can [access the full paper here](https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13310) or [download the full dataset and analysis scripts](https://osf.io/ev2xw/).\n\n## Further learning\n\nMake sure that you have completed the [core reading](https://eu.alma.exlibrisgroup.com/leganto/public/44YORK_INST/lists/60979364670001381?auth=SAML) for this week, and consider reading further resources to consolidate your understanding. As you'll have seen today, the code for actually running analyses is quite sparse(!) so there aren't too many interactive resources available. It's more important this week that you read around the decisions that need to be made along the way.\n\nIf you come to do meta-analysis in the future or want further details on the different steps that we have covered, then there is an excellent textbook available for free online: [Harrer et al. (2021). *Doing Meta-Analysis with R: A Hands-On Guide*](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/)*.*\n\n------------------------------------------------------------------------\n\n::: callout-important\n## Feedback!\n\nThese practical sessions are new this year. Please take a moment to rate the difficulty and length of these practical activities via [this survey](https://docs.google.com/forms/d/e/1FAIpQLSepox6r76dAkSDZRxybJwCvRX28M7BS1_81fGMybkqsvBPvRA/viewform?usp=publish-editor), and let us know of any issues you encountered or aspects you didn't understand (positive feedback is also welcome!). You will need to be logged in with your university account to respond, but your submission will be anonymous.\n\nFor more general questions about the practical, please post them on the [VLE Discussion Board](https://vle.york.ac.uk/ultra/courses/_116420_1/engagement/discussion/_6408724_1?view=discussions&disGroupId=default&courseId=_116420_1).\n:::\n",
    "supporting": [
      "ambr1_meta-analysis_files\\figure-docx"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}